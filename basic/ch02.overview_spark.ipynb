{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark_The_Definitive_Guide chapter02.overview spark\n",
    "> 이해 부족 추가공부 필요 <br>\n",
    "> - 2.7 트랜스포메이션, 셔플, 파이프라이닝 <br>\n",
    "> - 2.10 실행계획 파트 이해 부족 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['SPARK_HOME'] = '/Users/jeongmo/.sdkman/candidates/spark/current/'\n",
    "#os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'\n",
    "#os.system(\"pyspark -c spark.driver.bindAddress=127.0.0.1\")\n",
    "#from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"sparkJM\")\n",
    "    .master(\"local[1]\")\n",
    "    .config(\"spark.driver.host\",\"127.0.0.1\") #spark.driver.bindAdress Error solve\n",
    "    .config(\"spark.driver.bindAddress\",\"127.0.0.1\")  #spark.driver.bindAdress Error solve\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark 는 데이터를 읽어 로컬의 리스트/배열 형태로 변환함\n",
    "sdf = spark\\\n",
    "    .read\\\n",
    "    .option(\"inferSchema\",\"True\")\\\n",
    "    .option(\"header\",\"True\")\\\n",
    "    .csv(\"/Users/jeongmo/VSCodeLib/Study/data_from_git/Spark-The-Definitive-Guide/data/flight-data/csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=264), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=69)]\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#19 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#19 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=33]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#17,ORIGIN_COUNTRY_NAME#18,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/jeongmo/VSCodeLib/Study/data_from_git/Spark-The-Definitive..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take 메서드를 쓰면 해당 내용을 볼 수 있음\n",
    "# explain 메서드 쓰면 spark 의 실행계획 볼 수 있음\n",
    "print(sdf.take(3))\n",
    "sdf.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#19 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#19 ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [plan_id=43]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#17,ORIGIN_COUNTRY_NAME#18,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/jeongmo/VSCodeLib/Study/data_from_git/Spark-The-Definitive..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark 는 셔플 수행시, 기본적으로 200개의 셔플 파티션을 생성함. 셔플파티션 수 지정 옵션\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")  \n",
    "sdf.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "None\n",
      "1502\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sdf.printSchema())\n",
    "print(sdf.count())\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparkDataFrame 을 임시테이블(view)로 만들고, SQL 사용 가능!\n",
    "# spatkSQL 은 sparkDataFrame 코드와 같은 실행계획으로 컴파일 함 -> 둘 사이의 성능차이 없음\n",
    "sdf.createOrReplaceTempView(\"sdf_ex\")\n",
    "sdf_by_sql = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT DEST_COUNTRY_NAME, COUNT(1) as CNT\n",
    "    FROM sdf_ex\n",
    "    WHERE 1=1\n",
    "    GROUP BY DEST_COUNTRY_NAME\n",
    "    ORDER BY DEST_COUNTRY_NAME\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "sdf_1 = sdf.groupby(\"DEST_COUNTRY_NAME\")\\\n",
    "    .count()\\\n",
    "    .withColumnRenamed(\"count\",\"CNT\")\\\n",
    "    .orderBy(\"DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|   DEST_COUNTRY_NAME|CNT|\n",
      "+--------------------+---+\n",
      "|         Afghanistan|  3|\n",
      "|             Algeria|  3|\n",
      "|              Angola|  6|\n",
      "|            Anguilla|  6|\n",
      "| Antigua and Barbuda|  6|\n",
      "|           Argentina|  6|\n",
      "|               Aruba|  6|\n",
      "|           Australia|  6|\n",
      "|             Austria|  6|\n",
      "|          Azerbaijan|  4|\n",
      "|             Bahrain|  6|\n",
      "|            Barbados|  6|\n",
      "|             Belarus|  2|\n",
      "|             Belgium|  6|\n",
      "|              Belize|  6|\n",
      "|             Bermuda|  6|\n",
      "|             Bolivia|  6|\n",
      "|Bonaire, Sint Eus...|  6|\n",
      "|              Brazil|  6|\n",
      "|British Virgin Is...|  6|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [DEST_COUNTRY_NAME#17 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(DEST_COUNTRY_NAME#17 ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [plan_id=559]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 5), ENSURE_REQUIREMENTS, [plan_id=556]\n",
      "            +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_count(1)])\n",
      "               +- FileScan csv [DEST_COUNTRY_NAME#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/jeongmo/VSCodeLib/Study/data_from_git/Spark-The-Definitive..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "None\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [DEST_COUNTRY_NAME#17 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(DEST_COUNTRY_NAME#17 ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [plan_id=579]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 5), ENSURE_REQUIREMENTS, [plan_id=576]\n",
      "            +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_count(1)])\n",
      "               +- FileScan csv [DEST_COUNTRY_NAME#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/jeongmo/VSCodeLib/Study/data_from_git/Spark-The-Definitive..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sdf_by_sql.explain())\n",
    "print(sdf_1.explain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|          2348280|\n",
      "|           Canada|            49052|\n",
      "|           Mexico|            38075|\n",
      "|   United Kingdom|            10946|\n",
      "|            Japan|             9205|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf\\\n",
    "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "  .sum(\"count\")\\\n",
    "  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "  .sort(desc(\"destination_total\"))\\\n",
    "  .limit(5)\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#371L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#17,destination_total#371L])\n",
      "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[sum(count#19)])\n",
      "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 5), ENSURE_REQUIREMENTS, [plan_id=640]\n",
      "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_sum(count#19)])\n",
      "            +- FileScan csv [DEST_COUNTRY_NAME#17,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/jeongmo/VSCodeLib/Study/data_from_git/Spark-The-Definitive..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf\\\n",
    "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "  .sum(\"count\")\\\n",
    "  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "  .sort(desc(\"destination_total\"))\\\n",
    "  .limit(5)\\\n",
    "  .explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
