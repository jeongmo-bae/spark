{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import BytesIO,StringIO\n",
    "import jaydebeapi as jp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os \n",
    "os.chdir(\"/project_data/data_asset\")\n",
    "from hdfs.config import Config\n",
    "from useful.db_conn import * \n",
    "from useful.libs import *  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs client\n",
    "config = Config()\n",
    "config.add_section(config.global_section)\n",
    "section = 'cdp.alias'\n",
    "config.add_section(section)\n",
    "config.set(section, 'url', 'http://~~~~~~~~~~~~/;http://~~~~~~~~~~~~')\n",
    "config.set(section, 'user', '~~~~~~~~~~~~')\n",
    "client = config.get_client('cdp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark\n",
    "db = DB()\n",
    "sp = SparkDB('hive_table')\n",
    "sp.spark.conf.set('spark.sql.parquet.writeLegacyFormat', 'TRUE')\n",
    "db = DB()\n",
    "hconn = db.hconn\n",
    "hcurr = hconn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDataFrame = sp.spark.read.parquet(f'hdfs://datalake/user/~~~~~~~~~~~~/UploadData/ACRM/TSMFCRM70_ALL/20230315_crm70_all.parquet')\n",
    "sparkDataFrame.count()\n",
    "sparkDataFrame.rdd.getNumPartitions()\n",
    "#print(sparkDataFrame.rdd.getNumPartitions())\n",
    "sparkDataFrame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_name = [\n",
    "    'acmplcmpgnid',\n",
    "    'sendpshistidnfr',\n",
    "    'sendymd',\n",
    "    'msgcretncd',\n",
    "    'custidnfr',\n",
    "    'custmgtno',\n",
    "    'custnm',\n",
    "    'sendbrncd',\n",
    "    'sendempid',\n",
    "    'notimsgmdiadstcd',\n",
    "    'notimsgmdiadtalsdstcd',\n",
    "    'rcverinfoctnt',\n",
    "    'notisendstusdstcd',\n",
    "    'sendmsgtitlctnt',\n",
    "    'sendmsgctnt',\n",
    "    'sendsvcdsticctnt',\n",
    "    'sendsvcno',\n",
    "    'bzwkrceptidnfiid',\n",
    "    'syslastuno',\n",
    "    'flbcmdiactnt',\n",
    "    'flbcyn',\n",
    "    'newstarbnkgpushlagclsfidstcd',\n",
    "    'newstarbnkgpushmedmclsfidstcd',\n",
    "    'nttksendstusdstcd',\n",
    "    'nttksendrsultdstcd',\n",
    "    'nttktmpltid',\n",
    "    'frndtksendstusdstcd',\n",
    "    'frndtksendrsultdstcd'\n",
    "]\n",
    "for i ,col_name in enumerate(sparkDataFrame.columns) :\n",
    "    sparkDataFrame = sparkDataFrame.withColumnRenamed(col_name,new_column_name[i])\n",
    "\n",
    "sparkDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sparkDataFrame = sparkDataFrame.withColumn(\n",
    "    'p_yyyymmdd'\n",
    "    , F.col('sendymd')\n",
    ")\n",
    "sparkDataFrame = sparkDataFrame.drop(\"custnm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sparkDataFrame\n",
    "    .write\n",
    "    .option('header', True)\n",
    "    .partitionBy('p_yyyymmdd')\n",
    "    .format('parquet')\n",
    "    .mode('overwrite')\n",
    "    .save(f'hdfs://datalake/user/~~~~/UploadTable/tsmfcrm70_all')\n",
    ")\n",
    "\n",
    "client.list(f'/user/~~~~~~~~~~~~/UploadTable/tsmfcrm70_all/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_urls = client.list(base_url)[1:]\n",
    "data_urls\n",
    "\n",
    "base_url = '/user/~~~~~~~~~~~~/UploadTable/tsmfcrm70_all'\n",
    "data_urls = client.list(base_url)[1:]\n",
    "\n",
    "for p_yyyymmdd in data_urls:\n",
    "    time.sleep(0.1)\n",
    "    k1, v1 = p_yyyymmdd.split('=')[0], p_yyyymmdd.split('=')[1]\n",
    "\n",
    "    hcurr.execute(\n",
    "        f\"\"\"\n",
    "            LOAD DATA INPATH '/user/~~~~~~~~~~~~/UploadTable/tsmfcrm70_all/{p_yyyymmdd}' \n",
    "            OVERWRITE INTO TABLE project_dma.tsmfcrm70 \n",
    "            PARTITION({k1}='{v1}')\n",
    "        \"\"\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
